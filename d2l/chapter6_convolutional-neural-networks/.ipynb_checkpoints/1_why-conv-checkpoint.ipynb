{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# 从全连接层到卷积\n",
    "\n",
    "我们之前讨论的多层感知机十分适合处理表格数据，其中行对应样本，列对应特征。\n",
    "对于表格数据，我们寻找的模式可能涉及特征之间的交互，但是我们不能预先假设任何与特征交互相关的先验结构。\n",
    "此时，多层感知机可能是最好的选择，然而对于高维感知数据，这种缺少结构的网络可能会变得不实用。\n",
    "\n",
    "例如，在之前猫狗分类的例子中：假设我们有一个足够充分的照片数据集，数据集中是拥有标注的照片，每张照片具有百万级像素，这意味着网络的每次输入都有一百万个维度。\n",
    "根据我们在`subsec_parameterization-cost-fc-layers` 中对全连接层参数开销的讨论，\n",
    "即使将隐藏层维度降低到1000，这个全连接层也将有 $10^6 \\times 10^3 = 10^9$ 个参数。\n",
    "想要训练这个模型将不可实现，因为需要有大量的GPU、分布式优化训练的经验和超乎常人的耐心。\n",
    "\n",
    "有些读者可能会反对这个观点，认为要求百万像素的分辨率可能不是必要的。\n",
    "然而，即使分辨率减小为十万像素，使用1000个隐藏单元的隐藏层也可能不足以学习到良好的图像特征，在真实的系统中我们仍然需要数十亿个参数。\n",
    "此外，拟合如此多的参数还需要收集大量的数据。\n",
    "然而，如今人类和机器都能很好地区分猫和狗：这是因为图像中本就拥有丰富的结构，而这些结构可以被人类和机器学习模型使用。\n",
    "*卷积神经网络*（convolutional neural networks，CNN）是机器学习利用自然图像中一些已知结构的创造性方法。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 不变性\n",
    "\n",
    "想象一下，假设你想从一张图片中找到某个物体。\n",
    "合理的假设是：无论哪种方法找到这个物体，都应该和物体的位置无关。\n",
    "理想情况下，我们应该能够利用常识：猪通常不在天上飞，飞机通常不在水里游泳。\n",
    "我们可以从儿童游戏 ”沃尔多在哪里”（ :numref:`img_waldo` ）中得到灵感：\n",
    "在这个游戏中包含了许多充斥着活动的混乱场景，而沃尔多通常潜伏在一些不太可能的位置，读者的目标就是找出他。\n",
    "尽管沃尔多的装扮很有特点，但是在眼花缭乱的场景中找到他也如大海捞针。\n",
    "然而沃尔多的样子并不取决于他潜藏的地方，因此我们可以使用一个“沃尔多检测器”扫描图像。\n",
    "该检测器将图像分割成多个区域，并为每个区域包含沃尔多的可能性打分。\n",
    "卷积神经网络正是将*空间不变性*（spatial invariance）的这一概念系统化，从而基于这个模型使用较少的参数来学习有用的表示。\n",
    "\n",
    "![沃尔多游戏示例图。](../img/where-wally-walker-books.jpg)\n",
    ":width:`400px`\n",
    ":label:`img_waldo`\n",
    "\n",
    "\n",
    "现在，我们将上述想法总结一下，从而帮助我们设计适合于计算机视觉的神经网络结构：\n",
    "\n",
    "1. *平移不变性*（translation invariance）：不管检测对象出现在图像中的哪个位置，神经网络的前面几层应该对相同的图像区域具有相似的反应，即为“平移不变性”。\n",
    "1. *局部性*（locality）：神经网络的前面几层应该只探索输入图像中的局部区域，而不过度在意图像中相隔较远区域的关系，这就是“局部性”原则。最终，在后续神经网络，整个图像级别上可以集成这些局部特征用于预测。\n",
    "\n",
    "让我们看看这些原则是如何转化为数学表示的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 小结\n",
    "\n",
    "- 图像的平移不变性使我们以相同的方式处理局部图像，而不在乎它的位置。\n",
    "- 局部性意味着计算相应的隐藏表示只需一小部分局部图像像素。\n",
    "- 在图像处理中，卷积层通常比全连接层需要更少的参数，但依旧获得高效用的模型。\n",
    "- 卷积神经网络（CNN）是一类特殊的神经网络，它可以包含多个卷积层。\n",
    "- 多个输入和输出通道使模型在每个空间位置可以获取图像的多方面特征。\n",
    "\n",
    "\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 假设卷积层 :eqref:`eq_conv-layer` 覆盖的局部区域  $\\Delta = 0$  。在这种情况下，证明卷积内核为每组通道独立地实现一个全连接层。\n",
    "1. 为什么平移不变性可能也不是好主意呢？\n",
    "1. 当从图像边界像素获取隐藏表示时，我们需要思考哪些问题？\n",
    "1. 描述一个类似的音频卷积层的架构。\n",
    "1. 卷积层也适合于文本数据吗？为什么？\n",
    "1. 证明在 :eqref:`eq_2d-conv-discrete` 中， $f * g = g * f$  。\n",
    "\n",
    "[Discussions](https://discuss.d2l.ai/t/1846)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
